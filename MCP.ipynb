{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOny/2pXyal7rt2el6CxzbZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmarKhattab09/Model-Context-Protocol/blob/main/MCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FHMlV0coXbuI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "from google.colab import userdata\n",
        "token=userdata.get('hf_token')\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_id, use_auth_token=token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,device_map=\"cuda\",torch_dtype=\"auto\",trust_remote_code=True,use_auth_token=token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=token)"
      ],
      "metadata": {
        "id": "2X2SWbnZZiNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "ExGRjHF0Y9bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    {\"role\":\"user\",\"content\":\"Can you provide me instructions to create a cake\"}\n",
        "]\n",
        "generate_args={\n",
        "    \"max_new_tokens\": 2000,\n",
        "    \"return_full_text\":False,\n",
        "    \"do_sample\":False #Randomnize\n",
        "}"
      ],
      "metadata": {
        "id": "rHlKt8gWY7fh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages,**generate_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGvFaJbFeBI7",
        "outputId": "65ddbd6e-b238-48d8-9329-803997917226"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "kVSsr_GQeIE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0a7c82-fa29-4ad5-9a12-fa79e1e1d48d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a step-by-step guide to creating a basic cake:\n",
            "\n",
            "**Ingredients:**\n",
            "\n",
            "For a classic vanilla cake:\n",
            "\n",
            "* 2 cups all-purpose flour\n",
            "* 1 teaspoon baking powder\n",
            "* 1 teaspoon salt\n",
            "* 1 cup granulated sugar\n",
            "* 1/2 cup unsalted butter, softened\n",
            "* 2 large eggs\n",
            "* 2 teaspoons vanilla extract\n",
            "* 1 cup whole milk, at room temperature\n",
            "\n",
            "For the frosting (optional):\n",
            "\n",
            "* 1 cup unsalted butter, softened\n",
            "* 2 cups powdered sugar\n",
            "* 1 teaspoon vanilla extract\n",
            "* 2-4 tablespoons milk or heavy cream\n",
            "\n",
            "**Equipment:**\n",
            "\n",
            "* 9-inch round cake pan\n",
            "* Non-stick cooking spray or parchment paper\n",
            "* Electric mixer (stand or handheld)\n",
            "* Whisk\n",
            "* Measuring cups and spoons\n",
            "* Rubber spatula\n",
            "\n",
            "**Instructions:**\n",
            "\n",
            "**Step 1: Preheat and Prepare the Pan**\n",
            "\n",
            "1. Preheat your oven to 350°F (180°C).\n",
            "2. Spray the 9-inch round cake pan with non-stick cooking spray or line it with parchment paper.\n",
            "\n",
            "**Step 2: Mix the Dry Ingredients**\n",
            "\n",
            "1. In a medium bowl, whisk together the flour, baking powder, and salt. Set aside.\n",
            "\n",
            "**Step 3: Mix the Wet Ingredients**\n",
            "\n",
            "1. In a large mixing bowl, use an electric mixer to cream together the sugar and butter until light and fluffy.\n",
            "2. Beat in the eggs one at a time, allowing each egg to fully incorporate before adding the next.\n",
            "3. Beat in the vanilla extract.\n",
            "\n",
            "**Step 4: Combine the Wet and Dry Ingredients**\n",
            "\n",
            "1. With the mixer on low speed, gradually add the flour mixture to the wet ingredients in three parts, alternating with the milk, beginning and ending with the flour mixture. Beat just until combined.\n",
            "\n",
            "**Step 5: Pour the Batter into the Pan**\n",
            "\n",
            "1. Pour the cake batter into the prepared pan and smooth the top.\n",
            "\n",
            "**Step 6: Bake the Cake**\n",
            "\n",
            "1. Bake the cake for 25-30 minutes, or until a toothpick inserted into the center comes out clean.\n",
            "2. Remove the cake from the oven and let it cool in the pan for 5 minutes.\n",
            "\n",
            "**Step 7: Cool the Cake**\n",
            "\n",
            "1. Transfer the cake to a wire rack to cool completely.\n",
            "\n",
            "**Step 8: Make the Frosting (Optional)**\n",
            "\n",
            "1. Beat the softened butter in a large mixing bowl until light and fluffy.\n",
            "2. Gradually add the powdered sugar, beating until smooth and creamy.\n",
            "3. Beat in the vanilla extract.\n",
            "4. Add the milk or heavy cream, 1 tablespoon at a time, until the frosting reaches the desired consistency.\n",
            "\n",
            "**Step 9: Assemble and Frost the Cake (Optional)**\n",
            "\n",
            "1. Once the cake is completely cool, place one cake layer on a serving plate or cake stand.\n",
            "2. Spread a layer of frosting on top of the cake layer.\n",
            "3. Place the second cake layer on top of the frosting.\n",
            "4. Frost the top and sides of the cake with the remaining frosting.\n",
            "\n",
            "**Tips and Variations:**\n",
            "\n",
            "* To ensure the cake is evenly baked, rotate the pan halfway through the baking time.\n",
            "* To add flavor, try adding different extracts (e.g., almond, coconut, or lemon) or spices (e.g., cinnamon or nutmeg) to the batter.\n",
            "* To make a layer cake, repeat the process to create multiple layers.\n",
            "* To make a different type of cake, try substituting the flour with alternative flours (e.g., almond or coconut flour) or adding different ingredients (e.g., cocoa powder or nuts).\n",
            "\n",
            "I hope this helps! Let me know if you have any questions or if you'd like any variations on this recipe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESPONNSE FUNCTION"
      ],
      "metadata": {
        "id": "0xiLsJ4VelEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response_from_message_hf(pipe,messages,temperature=0):\n",
        "  sampling = False if temperature == 0 else True\n",
        "\n",
        "  generate_args={\n",
        "    \"max_new_tokens\": 2000,\n",
        "    \"return_full_text\":False,\n",
        "    \"do_sample\":sampling  #Randomnize\n",
        "    }\n",
        "  if temperature != 0:\n",
        "    generate_args['temperature']=temperature\n",
        "\n",
        "  output=pipe(messages,**generate_args)\n",
        "  return output[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "IoQdI9ELeoSv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PROMPT ENGINEERING\n",
        "messages=[\n",
        "    {\"role\":\"system\",\"content\":\"You are a Helpful AI assistant for a user who's name is ben\"},\n",
        "    {\"role\":\"user\",\"content\":\"what is my name\"}\n",
        "]"
      ],
      "metadata": {
        "id": "8Z9FV-aPfLUd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=get_response_from_message_hf(pipe,messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trD8kBYngAC-",
        "outputId": "057cc2c1-61d0-44e0-ea94-8ec58013e176"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2Ncu7IuDgBv7",
        "outputId": "8ad2800b-3b88-4c3a-ad50-d29698e142e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Ben.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "systemprompt = \"\"\"\n",
        "You are a helpful AI assistant that classifies customer support messages into one of those 3 categories\n",
        "1. Technical Issue\n",
        "2. Billing Industry\n",
        "3. General Feedback\n",
        "\"\"\"\n",
        "user_input = \"I love this Product\"\n",
        "\n",
        "messages = [\n",
        " {\"role\":\"system\",\"content\": systemprompt},\n",
        "  {\"role\":\"user\",\"content\":user_input}\n",
        "            ]"
      ],
      "metadata": {
        "id": "GIuFgmHlhssA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = get_response_from_message_hf(pipe,messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkJMebiIoPqa",
        "outputId": "b19a6696-caba-4fe0-ef8a-72d59f5c4ac3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qdkxdc0PombW",
        "outputId": "0d48d47e-4698-4f3a-861a-b4cbc2a1ae18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I would classify your message as: General Feedback'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def send_to_general_feedback(user_message):\n",
        "  print(\"sent to general feedback\")\n",
        "\n",
        "def send_to_billing_industry(user_message):\n",
        "  print(\"sent to billing industry\")\n",
        "\n",
        "def send_to_technical_issue(user_message):\n",
        "  print(\"sent to technical issue\")"
      ],
      "metadata": {
        "id": "KhpCZyL0os3U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"General Feedback\" in output:\n",
        "  send_to_general_feedback(user_input)\n",
        "if \"Billing Industry\" in output:\n",
        "  send_to_billing_industry(user_input)\n",
        "if \"Technical Issue\" in output:\n",
        "  send_to_technical_issue(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kAxyOASo3ge",
        "outputId": "3f100d3c-b023-415d-ea1f-e01086a6c428"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent to general feedback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE OF OUTPUT WHICH IS HARDER TO DETECT\n",
        "# OUT OF Technical Issue, Billing Inquiry and General Feedback, I classify it as General Feedback"
      ],
      "metadata": {
        "id": "XVybuijZpegW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "systemprompt = \"\"\"\n",
        "You are a helpful AI assistant that classifies customer support messages into one of those 3 categories\n",
        "1. Technical Issue\n",
        "2. Billing Industry\n",
        "3. General Feedback\n",
        "\n",
        "the output should be in a json format like this\n",
        "{\n",
        "  \"category\":\"use one from the 3 categories above\"\n",
        "}\n",
        "\"\"\"\n",
        "user_input = \"I love this Product\"\n",
        "\n",
        "messages = [\n",
        " {\"role\":\"system\",\"content\": systemprompt},\n",
        "  {\"role\":\"user\",\"content\":user_input}\n",
        "            ]"
      ],
      "metadata": {
        "id": "EQok8LnZpry8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = get_response_from_message_hf(pipe,messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eBIYIqrp9jX",
        "outputId": "14607421-9691-41b0-daec-1241ea381626"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "op=json.loads(output)\n",
        "op['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qYGcrXXGp-QB",
        "outputId": "c362d1d5-1f2f-4582-ffa2-c577b6737ea7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'General Feedback'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if op['category'] == \"General Feedback\":\n",
        "  send_to_general_feedback(user_input)\n",
        "if op['category'] == \"Billing Industry\":\n",
        "  send_to_billing_industry(user_input)\n",
        "if op['category'] == \"Technical Issue\":\n",
        "  send_to_technical_issue(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzgHFjSvqMxT",
        "outputId": "6d713a37-e467-4c58-eb40-27b1edd018b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent to general feedback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function_dict = {\n",
        "    \"General Feedback\":send_to_general_feedback,\n",
        "    \"Billing Industry\":send_to_billing_industry,\n",
        "    \"Technical Issue\":send_to_technical_issue\n",
        "}\n",
        "function_dict[op['category']](user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTjYTSyLqhrl",
        "outputId": "d4d4388b-9738-41f5-f65a-cfa55fb2aee7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent to general feedback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_Tt-HlwraT4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCP CLIENT STEPS:\n",
        "- Know what functions are available to use\n",
        "- Determining if the user proompts requires the output of one of those function, this is call prompt intent\n",
        "- lastly run the function and append its output to the llm"
      ],
      "metadata": {
        "id": "FR2OvdZDralr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Prompt Intent\n",
        "## 1. TASK\n",
        "## 2. Classifictions\n",
        "## 3. Output Form"
      ],
      "metadata": {
        "id": "NknZ_3voruvF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MCP DEVELOPMENT"
      ],
      "metadata": {
        "id": "4ehTpYKLr_3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_name():\n",
        "  return \"umar\"\n",
        "\n",
        "def get_user_age():\n",
        "  return 22"
      ],
      "metadata": {
        "id": "X7iVY8wysDE2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools={\n",
        "    \"get_user_name\": {\n",
        "        \"description\": \"This tools returns the name of the user\",\n",
        "        \"tool\": get_user_name,\n",
        "        \"parameters\": None\n",
        "\n",
        "    },\n",
        "    \"get_user_age\": {\n",
        "        \"description\": \"This tools returns the age of the user\",\n",
        "        \"tool\": get_user_age,\n",
        "        \"parameters\": None,\n",
        "    }}"
      ],
      "metadata": {
        "id": "3Emf43KXsLyx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools_str = str(tools)"
      ],
      "metadata": {
        "id": "50u0sXkuuV0n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "system_prompt= f\"\"\"\n",
        "You are a system that reads each user message and decides whether to invoke one of the available tools to generate the answer.\n",
        "Not every message requires a tool. Only select a tool when it directly helps respond to the user’s request.\n",
        "\n",
        "The full list of available tools is provided as JSON here:\n",
        "{tools_str}\n",
        "\n",
        "When you respond, follow this pattern:\n",
        "  1.\n",
        "      Reason whether any of the provided tools is a good fit or not.\n",
        "      Reasoning should be 2 sentences or less\n",
        "      Reasoning should end with a new line then \"no tool is needed\" If no tool is a good fit or if no tools are relevant.\n",
        "  2.\n",
        "    On its own line, write: \"Json Output\"\n",
        "  3.\n",
        "    Choose one the following two options:\n",
        "    a. If you determine no tool is needed, simply return an empty json like so {{}}\n",
        "    b. If a tool is needed: provide a JSON object with exactly two keys like the one bellow:\n",
        "      {{\n",
        "        \"tool_name\":\"\" # put here the name of the tool,\n",
        "        \"parameters\":{{}} # parameters in a proper json format, this is going to be empty if there are no parameters.\n",
        "      }}\n",
        "\"\"\"\n",
        "\n",
        "user_message = \"What's my name?\"\n",
        "\n",
        "messages =[\n",
        "    {\"role\":\"system\",\"content\":system_prompt},\n",
        "    {\"role\":\"user\",\"content\":user_message}\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "W0q9rdKttg6E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = get_response_from_message_hf(pipe,messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU2Ye31pwQeM",
        "outputId": "aeee6179-2ce5-453e-d5d6-a1188d3b9321"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgccpXEKwRR3",
        "outputId": "489e234d-6a6e-492b-c303-4781c1d08f98"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The 'get_user_name' tool is a good fit for this request as it directly returns the name of the user.\n",
            " \n",
            "Json Output\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"get_user_name\",\n",
            "  \"parameters\": {}\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages =[\n",
        "    {\"role\":\"system\",\"content\":system_prompt},\n",
        "    {\"role\":\"user\",\"content\":\"Capital of France?\"}\n",
        "    ]\n",
        "output = get_response_from_message_hf(pipe,messages)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-pzZ9xww0Pf",
        "outputId": "002fabaf-44a8-4b4f-f138-cb3045340016"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The 'get_user_name' tool is not a good fit for this request as it is not relevant to finding the capital of France. The 'get_user_age' tool is also not a good fit for this request for the same reason.\n",
            "\n",
            "Json Output\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"get_user_name\",\n",
            "  \"parameters\": {}\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages =[\n",
        "    {\"role\":\"system\",\"content\":system_prompt},\n",
        "    {\"role\":\"user\",\"content\":\"What is my age?\"}\n",
        "    ]\n",
        "output = get_response_from_message_hf(pipe,messages)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28NzsmgYxFiy",
        "outputId": "8b95aeb8-d581-4031-e033-4913ce38364a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The 'get_user_age' tool is a good fit for this request because it directly returns the user's age, which is the information the user is asking for. This tool is the most relevant to the user's query.\n",
            "\n",
            "Json Output\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"get_user_age\",\n",
            "  \"parameters\": {}\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_message_intent(pipe,tools,user_message):\n",
        "    tool_list_str = str(tools)\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a system that reads each user message and decides whether to invoke one of the available tools to generate the answer.\n",
        "    Not every message requires a tool.  Only select a tool when it directly helps respond to the user’s request.\n",
        "\n",
        "    The full list of available tools is provided as JSON here:\n",
        "    {tool_list_str}\n",
        "\n",
        "    When you respond, follow this pattern:\n",
        "    1.\n",
        "        Reason whether any of the provded tools is a good fit or not.\n",
        "        Reasoning should be 2 sentences or less\n",
        "        Reasoning should end with a new line then \"no tool is needed\" If no tool is a good fit or if no tools are relevant.\n",
        "    2.\n",
        "      On its own line, write: \"Json Output\"\n",
        "    3.\n",
        "      Choose one the following two options:\n",
        "      a. If you determine no tool is needed, simply return an empty json like so {{}}\n",
        "      b. If a tool is needed: provide a JSON object with exactly two keys like the one bellow:\n",
        "        {{\n",
        "          \"tool_name\":\"\" # put here the name of the tool,\n",
        "          \"parameters\":{{}} # parameters in a proper json format, this is going to be empty if there are no parameters.\n",
        "        }}\n",
        "\n",
        "    don't number the output 1,2,3 or a,b.\n",
        "    \"\"\"\n",
        "\n",
        "    messages =[\n",
        "      {\"role\":\"system\",\"content\":system_prompt},\n",
        "      {\"role\":\"user\",\"content\":user_message}\n",
        "    ]\n",
        "\n",
        "    output = get_response_from_message_hf(pipe,messages)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "7KWu4deJxNpQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = get_message_intent(pipe,tools,\"What's my name?\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w19EyE5xOI8",
        "outputId": "527a30e6-9721-4866-d121-17a59da3537c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reason: The 'get_user_name' tool is a good fit for this request as it directly returns the name of the user. This tool is relevant to the user's request.\n",
            "\n",
            "Json Output\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"get_user_name\",\n",
            "  \"parameters\": {}\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = json.loads(((output.lower().split('json output')[1]).strip()))\n",
        "json_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R405NGxAycvw",
        "outputId": "4685247b-6c86-48aa-8195-ada8187b98f0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tool_name': 'get_user_name', 'parameters': {}}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function_to_run=(json_output['tool_name'])\n",
        "parameters = json_output['parameters']"
      ],
      "metadata": {
        "id": "abClGmTbzwaA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy_YnEZiz4Sx",
        "outputId": "5616fd18-49bb-4396-e168-04492dc14a47"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'get_user_name': {'description': 'This tools returns the name of the user',\n",
              "  'tool': <function __main__.get_user_name()>,\n",
              "  'parameters': None},\n",
              " 'get_user_age': {'description': 'This tools returns the age of the user',\n",
              "  'tool': <function __main__.get_user_age()>,\n",
              "  'parameters': None}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = tools[function_to_run]['tool'](**parameters)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "De_aWNifz8gX",
        "outputId": "95fded79-83fd-4051-af23-bd8f04712b5a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'umar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_run_output = f\"\"\"This is output when we ran {function_to_run} that does this {tools[function_to_run]['description']}\n",
        "    Tool Output: {result}\n",
        "    \"\"\"\n",
        "print(tool_run_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFnyS3Ox0Wlz",
        "outputId": "8857eded-1c26-4c44-aa6c-35cf9ee56b6c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is output when we ran get_user_name that does this This tools returns the name of the user\n",
            "    Tool Output: umar\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Veah697l1fl3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BemLuSPQ1for"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VXI6hyc1bKb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"Whats my name\"\n",
        "messages = [\n",
        "    {\"role\":\"user\",\"content\":user_message+\"\"\"\n",
        "    That tool’s output will be:\n",
        "        Data Output – Information you should incorporate into your answer as factual content.\n",
        "\n",
        "      When you compose your reply, follow these steps:\n",
        "\n",
        "      1. Inspect the tool output\n",
        "          weave it into your response as part of your knowledge\n",
        "\n",
        "      2. Respond appropriately\n",
        "         use it to directly inform or support your answer.\n",
        "\n",
        "    \"\"\"+tool_run_output}\n",
        "    ]"
      ],
      "metadata": {
        "id": "Lf8q1zd71bNf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_facing_response = get_response_from_message_hf(pipe,messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdjlsdqr1dOA",
        "outputId": "dab9198f-d83d-4214-fa72-c6a36e5576ae"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_facing_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEXvqVbz1ioJ",
        "outputId": "0eb927e5-4ac2-42fb-edf0-0c87d2d08f18"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the tool output, it appears that your name is Umar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"Capital of France?\"\n",
        "messages = [\n",
        "    {\"role\":\"user\",\"content\":user_message+\"\"\"\n",
        "    That tool’s output will be:\n",
        "        Data Output – Information you should incorporate into your answer as factual content.\n",
        "\n",
        "      When you compose your reply, follow these steps:\n",
        "\n",
        "      1. Inspect the tool output\n",
        "          weave it into your response as part of your knowledge\n",
        "\n",
        "      2. Respond appropriately\n",
        "         use it to directly inform or support your answer.\n",
        "\n",
        "    \"\"\"+tool_run_output}\n",
        "    ]\n",
        "user_facing_response = get_response_from_message_hf(pipe,messages)\n",
        "print(user_facing_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "555lxfpY1uxl",
        "outputId": "fcc1ec4f-cdf6-42c8-88d5-cfe970330cae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tools(tools,message_intent):\n",
        "  json_output = json.loads(((message_intent.lower().split('json output')[1]).strip()))\n",
        "\n",
        "  if len(json_output)==0:\n",
        "    return \"\"\n",
        "\n",
        "  function_to_run = json_output['tool_name']\n",
        "  parameters = json_output['parameters']\n",
        "  result = tools[function_to_run][\"tool\"](**parameters)\n",
        "\n",
        "  output_response = f\"\"\"This is output when we ran {function_to_run} that does this {tools[function_to_run]['description']}\n",
        "    Tool Output: {result}\n",
        "    \"\"\"\n",
        "\n",
        "  return output_response"
      ],
      "metadata": {
        "id": "S89VjxA02Iuz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3P5h28uR2Kk2",
        "outputId": "299c56aa-2b0c-45f6-e3bb-af0e89ff4f29"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reason: The \\'get_user_name\\' tool is a good fit for this request as it directly returns the name of the user. This tool is relevant to the user\\'s request.\\n\\nJson Output\\n\\n{\\n  \"tool_name\": \"get_user_name\",\\n  \"parameters\": {}\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_run_output = run_tools(tools,output)\n",
        "print(tool_run_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4DSVhOt2O7u",
        "outputId": "e080cb6a-7cdc-4585-a663-73fbff3ea88d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is output when we ran get_user_name that does this This tools returns the name of the user\n",
            "    Tool Output: umar\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Client:\n",
        "  def __init__(self,tools):\n",
        "    self.tools=tools\n",
        "    self.strtools = str(tools)\n",
        "\n",
        "\n",
        "\n",
        "  def get_message_intent(self,pipe,user_message):\n",
        "    system_prompt = f\"\"\"\n",
        "      You are a system that reads each user message and decides whether to invoke one of the available tools to generate the answer.\n",
        "      Not every message requires a tool.  Only select a tool when it directly helps respond to the user’s request.\n",
        "\n",
        "      The full list of available tools is provided as JSON here:\n",
        "        {self.strtools}\n",
        "\n",
        "      When you respond, follow this pattern:\n",
        "      1.\n",
        "          Reason whether any of the provded tools is a good fit or not.\n",
        "          Reasoning should be 2 sentences or less\n",
        "          Reasoning should end with a new line then \"no tool is needed\" If no tool is a good fit or if no tools are relevant.\n",
        "      2.\n",
        "        On its own line, write: \"Json Output\"\n",
        "      3.\n",
        "        Choose one the following two options:\n",
        "        a. If you determine no tool is needed, simply return an empty json like so {{}}\n",
        "        b. If a tool is needed: provide a JSON object with exactly two keys like the one bellow:\n",
        "          {{\n",
        "            \"tool_name\":\"\" # put here the name of the tool,\n",
        "            \"parameters\":{{}} # parameters in a proper json format, this is going to be empty if there are no parameters.\n",
        "          }}\n",
        "\n",
        "      don't number the output 1,2,3 or a,b.\n",
        "      \"\"\"\n",
        "    messages =[\n",
        "        {\"role\":\"system\",\"content\":system_prompt},\n",
        "        {\"role\":\"user\",\"content\":user_message}\n",
        "      ]\n",
        "\n",
        "    output = get_response_from_message_hf(pipe,messages)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def run_tools(self,message_intent):\n",
        "    json_output = json.loads(((message_intent.lower().split('json output')[1]).strip()))\n",
        "\n",
        "    if len(json_output)==0:\n",
        "      return \"\"\n",
        "\n",
        "    function_to_run = json_output['tool_name']\n",
        "    parameters = json_output['parameters']\n",
        "    result = self.tools[function_to_run][\"tool\"](**parameters)\n",
        "\n",
        "    output_response = f\"\"\"This is output when we ran {function_to_run} that does this {tools[function_to_run]['description']}\n",
        "      Tool Output: {result}\n",
        "      \"\"\"\n",
        "\n",
        "    return output_response\n"
      ],
      "metadata": {
        "id": "8qCrYtdNMWsJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Host\n",
        "class Host():\n",
        "  def __init__(self,llm_model,client):\n",
        "    self.client = client\n",
        "    self.llm_model=llm_model\n",
        "    self.message_history=[]\n",
        "\n",
        "  def get_user_response(self,user_message):\n",
        "    original_user_message= user_message\n",
        "\n",
        "    intent_output = self.client.get_message_intent(self.llm_model,user_message)\n",
        "    tool_output = self.client.run_tools(intent_output)\n",
        "\n",
        "\n",
        "    if tool_output.strip() !=\"\" and  tool_output.strip() !=\"no tool is needed\":\n",
        "      message_tool_attachement = \"\"\"\n",
        "      That tool’s output will be one of two kinds:\n",
        "        1. Data Output – Information you should incorporate into your answer as factual content.\n",
        "        2. Status Output – A message indicating success or failure of the tool or an opperation. In this case, simply report the status to the user; do not suggest or describe how to run the tool.\n",
        "\n",
        "      When you compose your reply, follow these steps:\n",
        "\n",
        "      1. Inspect the tool output\n",
        "          a. If it’s data, weave it into your response as part of your knowledge.\n",
        "          b. If it’s a function status message, communicate that status to the user.\n",
        "\n",
        "      2. Respond appropriately\n",
        "        a. For data: use it to directly inform or support your answer.\n",
        "        b. For function status: acknowledge success or explain the error without offering tool-invocation instructions.\n",
        "\n",
        "      \"\"\"+tool_output\n",
        "\n",
        "      user_message+= message_tool_attachement\n",
        "\n",
        "    self.message_history.append({\"role\":\"user\",\"content\":user_message})\n",
        "\n",
        "    user_facing_output = get_response_from_message_hf(self.llm_model,self.message_history)\n",
        "\n",
        "    self.message_history[-1]['content'] = original_user_message\n",
        "    self.message_history.append({\"role\":\"assistant\",\"content\":user_facing_output})\n",
        "    return user_facing_output\n"
      ],
      "metadata": {
        "id": "TBIN_eIJNo2T"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JJ4XNXsQU3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resources\n",
        "def get_user_name():\n",
        "  return \"Ben\"\n",
        "def get_user_age():\n",
        "  return 28\n",
        "\n",
        "# tool\n",
        "def write_to_file(file_path,content):\n",
        "  with open(file_path,'w') as f:\n",
        "    f.write(content)\n",
        "  return \"File written successfully in the specified path: \"+file_path"
      ],
      "metadata": {
        "id": "pYGtzWRUPl5t"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student Database\n",
        "student_database_json = {\n",
        "    \"s001\": {\n",
        "        \"name\": \"Alice\",\n",
        "        \"GPA\": 3.2,\n",
        "        \"major\": \"Computer Science\",\n",
        "        \"Courses\" : [\"programming 101\",\"networks\",\"AI\", \"digital logic\"]\n",
        "    },\n",
        "    \"s002\": {\n",
        "        \"name\": \"Bob\",\n",
        "        \"GPA\": 3.8,\n",
        "        \"major\": \"Mathematics\",\n",
        "        \"Courses\" : [\"programming 101\",\"math 101\",\"english 201\", \"linear algebra\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_student_gpa(student_id):\n",
        "  if student_id in student_database_json:\n",
        "    return student_database_json[student_id][\"GPA\"]\n",
        "  else:\n",
        "    return \"Student not found\"\n",
        "\n",
        "def get_student_major(student_id):\n",
        "  if student_id in student_database_json:\n",
        "    return student_database_json[student_id][\"major\"]\n",
        "  else:\n",
        "    return \"Student not found\"\n",
        "\n",
        "def get_student_courses(student_id):\n",
        "  if student_id in student_database_json:\n",
        "    return student_database_json[student_id][\"Courses\"]\n",
        "  else:\n",
        "    return \"Student not found\"\n",
        "\n",
        "def add_student(student_id,student_name,student_gpa,student_major,student_courses):\n",
        "  student_database_json[student_id] = {\n",
        "      \"name\": student_name,\n",
        "      \"GPA\": student_gpa,\n",
        "      \"major\": student_major,\n",
        "      \"Courses\" : student_courses }\n",
        "\n",
        "  return \"Student added successfully\"\n",
        "\n",
        "def edit_student_gpa(student_id,new_gpa):\n",
        "  if student_id in student_database_json:\n",
        "    student_database_json[student_id][\"GPA\"] = new_gpa\n",
        "    return \"GPA updated successfully\"\n",
        "  else:\n",
        "    return \"Student not found\"\n"
      ],
      "metadata": {
        "id": "_yT5hM7RQYeI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = {\n",
        "    \"get_user_name\": {\n",
        "        \"description\": \"This tool returns the user's name\",\n",
        "        \"tool\":get_user_name,\n",
        "        \"parameters\": None,\n",
        "    },\n",
        "    \"get_user_age\": {\n",
        "        \"description\": \"This tool returns the user's age\",\n",
        "        \"tool\":get_user_age,\n",
        "        \"parameters\":None,\n",
        "    },\n",
        "    \"write_to_file\": {\n",
        "        \"description\": \"This tool writes content to a file\",\n",
        "        \"tool\":write_to_file,\n",
        "        \"parameters\":{'file_path':'the path where the file should be saved', 'content':'the content that should be written in the file'},\n",
        "    },\n",
        "\n",
        "    \"get_student_gpa\": {\n",
        "        \"description\": \"This tool returns the student's GPA\",\n",
        "        \"tool\":get_student_gpa,\n",
        "        \"parameters\":{'student_id':'the id of the student'},\n",
        "    },\n",
        "    \"get_student_major\": {\n",
        "        \"description\": \"This tool returns the student's major\",\n",
        "        \"tool\":get_student_major,\n",
        "        \"parameters\":{'student_id':'the id of the student'},\n",
        "    },\n",
        "    \"get_student_courses\": {\n",
        "        \"description\": \"This tool returns the student's courses\",\n",
        "        \"tool\":get_student_courses,\n",
        "        \"parameters\":{'student_id':'the id of the student'},\n",
        "    },\n",
        "    \"add_student\": {\n",
        "        \"description\": \"This tool adds a student to the database\",\n",
        "        \"tool\":add_student,\n",
        "        \"parameters\":{'student_id':'the id of the student', 'student_name':'the name of the student', 'student_gpa':'the GPA of the student', 'student_major':'the major of the student', 'student_courses':'the courses of the student'},\n",
        "    },\n",
        "    \"edit_student_gpa\": {\n",
        "        \"description\": \"This tool edits the student's GPA\",\n",
        "        \"tool\":edit_student_gpa,\n",
        "        \"parameters\":{'student_id':'the id of the student', 'new_gpa':'the new GPA of the student'},\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Bf_CURz-QdwM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoZJvJszQgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client(tools)\n",
        "host = Host(pipe,client)"
      ],
      "metadata": {
        "id": "AuOLzJvnQgZE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_facing_output = host.get_user_response(\"What's my name?\")\n",
        "print(user_facing_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfUy4OY5Qkm3",
        "outputId": "4210f0c8-38d0-40b7-8752-bbc42a09db7c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Ben.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_facing_output = host.get_user_response(\"Write 'hello there' in a text file in this path 'greetings.txt' \")\n",
        "print(user_facing_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR-bldrfQsg_",
        "outputId": "d70b2187-e195-426a-ff43-a4fee22c55f6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tool output indicates that the file was written successfully. Here's a response:\n",
            "\n",
            "The file \"greetings.txt\" has been written to the specified path. The content of the file is \"hello there\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_facing_output = host.get_user_response(\"What's the major of the student with id s002?\")\n",
        "print(user_facing_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBN8yvCFQ5-v",
        "outputId": "024565db-352c-44ec-919c-d3f342ce3cda"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The major of the student with id s002 is Mathematics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_facing_output = host.get_user_response(\"Edit the GPA of student s001 to 3.8\")\n",
        "print(user_facing_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDl36BvwQ_l5",
        "outputId": "c4c23091-869e-4f55-9cc9-38d68bfadd12"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The GPA of student s001 has been updated to 3.8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_database_json['s001']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVgulq2cRCIG",
        "outputId": "037fef47-d35d-4437-d1ce-f275fedd6204"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Alice',\n",
              " 'GPA': '3.8',\n",
              " 'major': 'Computer Science',\n",
              " 'Courses': ['programming 101', 'networks', 'AI', 'digital logic']}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}